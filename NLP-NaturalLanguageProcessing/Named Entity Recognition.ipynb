{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa712bc",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde03eb",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this walkthrough, we will be using a Hidden Markov Model to perform Named Entity Recognition to gain experience with sequence labeling.\n",
    "\n",
    "**Please note: The data that we will be using is relatively unfiltered, real world data that comes from tweets. It contains some vulgar language that has not been filtered out.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94c724",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "The training data in `data/train` is in the following format:\n",
    "\n",
    "    Greek B-other\n",
    "    Festival I-other\n",
    "    at O\n",
    "    St B-facility\n",
    "    Johns I-facility\n",
    "    before O\n",
    "    ASPEN B-geo-loc\n",
    "    \n",
    "    Ew O\n",
    "    wait O\n",
    "    ...\n",
    "\n",
    "It is organized with two whitespace-seperated columns as seen above. The first column contains the word and the second column contains that word's corresponding BIO tag. Blank lines seperate one sentence from another.\n",
    "\n",
    "The code below reads in the training data and stores it as a list called `train` in the following format:\n",
    "\n",
    "    [\n",
    "    [\"Greek Festival at St Johns before ASPEN\", [\"B-other\", \"I-other\", \"O\", \"B-facility\", \"I-facility\", \"O\", \"B-geo-loc\"]],\n",
    "    [\"Ew wait ...\", [\"O\", \"O\", ...]],\n",
    "    ...\n",
    "    ]\n",
    "\n",
    "Each element in the list should be a list of length 2. The first element of this list should be the sentence as a space-seperated string and the second element in this list should be a list of the associated tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba3cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [' '.join(i.split('\\n')).replace('\\t', ' ').split() for i in open('data/train', 'r').read().split('\\n\\n')]\n",
    "train = [[\" \".join(i[::2]),i[1::2]] for i in train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3dd80",
   "metadata": {},
   "source": [
    "First, we create a list called `tags` containing all of the unique tags found in the training data and a list called `words` containing all of the unique words found in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a39973",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30aa9de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 46235, Word Types: 10553, Tag Types: 21 \n"
     ]
    }
   ],
   "source": [
    "### ENTER CODE HERE ###\n",
    "\n",
    "# Print the results\n",
    "print(f\"Tokens: {sum([len(i[1]) for i in train])}, Word Types: {len(words)}, Tag Types: {len(tags)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764afdf0",
   "metadata": {},
   "source": [
    "### Hidden Markov Model\n",
    "\n",
    "Recall that a Hidden Markov Model (HMM) is a type of statistical model that's used for sequential data. HMMs assume that an underlying process is governed by a Markov process with unobservable (i.e., hidden) states. The model is called \"hidden\" because the states and state transitions aren't directly observable; instead, we observe some output that is generated from each state.\n",
    "\n",
    "The Markov assumption means that the probability of transitioning to any particular state depends solely on the current state and time, independent of the sequence of states that preceded it. This sequence property makes Markov models particularly useful for algorithms like Viterbi, which finds the most likely sequence of hidden states given the sequence of observations.\n",
    "\n",
    "We will use the HMM in this task for part-of-speech (POS) tagging. In POS tagging, the goal is to label each word in a sentence with its appropriate part of speech, such as noun, verb, adjective, etc.\n",
    "\n",
    "First, we will add the following words to the `words` list:\n",
    "- `<B>` to denote the beginning of a sentence\n",
    "- `<E>` to denote the end of a sentence\n",
    "- `<UNK>` for any unseen words\n",
    "\n",
    "And the following tags to the `tags` list:\n",
    "- `<B>` to tag the `<B>` word\n",
    "- `<E>` to tag the `<E>` word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0cadb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ff557",
   "metadata": {},
   "source": [
    "Now, we will create an `HMM` class with the following functions:\n",
    "\n",
    "- **'train'** function: This function takes one 'train data' set, creates one list of \"tags\", one list of \"words\" and counts each tag, each tag-tag pair, and each word-tag pair occurance in the training data. \n",
    " \n",
    "- **'transition_proba'** function: That is the 'Transition Probabilities' Function. It takes on **tag-tag** pair and returns the relative frequency estimation for this given **tag-tag** pair. Recall that the relative frequency estimation for an HMM involves both **transition probabilities** (tag1-tag2 pair count / tag1 count) and **emission probabilities** (word-tag pair counts / tag count), but for this part we will only implement the 'Transition Probabilities'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75063e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.words=set()\n",
    "        self.tags=set()\n",
    "        \n",
    "        self.tag_counts = {}\n",
    "        self.tagtag_counts = {}\n",
    "        self.wordtag_counts = {}\n",
    "        \n",
    "    def train(self, train, words, tags):\n",
    "        import itertools\n",
    "        self.tags = tags\n",
    "        self.words = words\n",
    "\n",
    "        self.tag_counts          = {(i): 0 for i in self.tags}      \n",
    "        self.tagtag_counts       = {(i[0], i[1]): 0 for i in itertools.product(self.tags, self.tags)}\n",
    "        self.wordtag_counts      = {(i[0], i[1]): 0 for i in itertools.product(self.words, self.tags)}\n",
    "        \n",
    "        \n",
    "        for sentence_tag in train: #for each sentence\n",
    "            xtags = sentence_tag[1].copy()\n",
    "            xtags.insert(0,'<B>')\n",
    "            xtags.append('<E>')\n",
    "\n",
    "\n",
    "            xsentence = sentence_tag[0].split()\n",
    "            xsentence.insert(0,'<B>')\n",
    "            xsentence.append('<E>')\n",
    "            \n",
    "            #count tags\n",
    "            ### ENTER CODE HERE ###    \n",
    "  \n",
    "            #count tags-tags\n",
    "            ### ENTER CODE HERE ###\n",
    "\n",
    "            #count word-tag       \n",
    "            ### ENTER CODE HERE ###\n",
    "         \n",
    "            \n",
    "    def transition_proba(self, tag_tag): #transitions (tag1-tag2 pair count / tag1 count)  \n",
    "        ### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f3fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e07e007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(B-person | O) = 0.009114583333333334\n",
      "p(B-person | B-person) = 0.0\n",
      "p(I-person | B-person) = 0.4375\n",
      "p(B-person | I-person) = 0.0\n",
      "p(I-person | I-person) = 0.08411214953271028\n",
      "p(O | I-person) = 0.8878504672897196\n"
     ]
    }
   ],
   "source": [
    "print(\"p(B-person | O) = \" + str(hmm.transition_proba(('O', 'B-person'))))\n",
    "print(\"p(B-person | B-person) = \" + str(hmm.transition_proba(('B-person', 'B-person'))))\n",
    "print(\"p(I-person | B-person) = \" + str(hmm.transition_proba(('B-person', 'I-person'))))\n",
    "print(\"p(B-person | I-person) = \" + str(hmm.transition_proba(('I-person', 'B-person'))))\n",
    "print(\"p(I-person | I-person) = \" + str(hmm.transition_proba(('I-person', 'I-person'))))\n",
    "print(\"p(O | I-person) = \" + str(hmm.transition_proba(('I-person', 'O'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becef46d",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\n",
    "In the context of Hidden Markov Models (HMMs), \"smoothing\" refers to a method used to estimate the \"hidden\" state of a system at a particular point in time, given all of the observed data (i.e., not just data up to that point, but data that comes after it as well). It's called \"smoothing\" because it uses future data to \"smooth\" out the prediction for the current state.\n",
    "\n",
    "We will copy our HMM class from above and modify it by smoothing the emission probabilities. This involves the following: \n",
    "\n",
    "- Creating a new dictionary called `self.smoothed_tag_counts` for the smoothed tag counts. In the Train Function, we will use it to count the Tags and add 0.1 to each 'word' of the 'word-tag' counts.\n",
    "- Initializing the wordtag_counts with 0.1 instead of 0\n",
    "- Creating an Emission Probabilities function called `emission_proba` that returns the relative frequency estimation for a given 'word-tag' pair. In this function, if the 'word' from the 'word-tag' is unknown (not seen before in the training), we will return the emission probability of \"\\<UNK\\>\"-tag instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174bef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class smoothHMM():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.words=set()\n",
    "        self.tags=set()\n",
    "        \n",
    "        self.tag_counts = {}\n",
    "        self.tagtag_counts = {}\n",
    "        self.wordtag_counts = {}\n",
    "        self.smoothed_tag_counts = {}\n",
    "        \n",
    "    def train(self, train, words, tags):\n",
    "        import itertools\n",
    "        self.tags = tags\n",
    "        self.words = words\n",
    "\n",
    "        self.tag_counts          = {(i): 0 for i in self.tags}      \n",
    "        self.smoothed_tag_counts = {(i): 0 for i in self.tags} \n",
    "        self.tagtag_counts       = {(i[0], i[1]): 0 for i in itertools.product(self.tags, self.tags)}\n",
    "        self.wordtag_counts      = {(i[0], i[1]): 0.1 for i in itertools.product(self.words, self.tags)}\n",
    "        \n",
    "        ### ENTER CODE HERE ###\n",
    "        \n",
    "        for sentence_tag in train: #for each sentence\n",
    "            xtags = sentence_tag[1].copy()\n",
    "            xtags.insert(0,'<B>')\n",
    "            xtags.append('<E>')\n",
    "\n",
    "\n",
    "            xsentence = sentence_tag[0].split()\n",
    "            xsentence.insert(0,'<B>')\n",
    "            xsentence.append('<E>')\n",
    "            \n",
    "            #count tags\n",
    "            for xtag in xtags:\n",
    "                self.tag_counts[xtag]+=1      \n",
    "                ### ENTER CODE HERE ###\n",
    "\n",
    "            #count tags-tags\n",
    "            for i in range(len(xtags)-1):\n",
    "                self.tagtag_counts[(xtags[i],xtags[i+1])]+=1  \n",
    "\n",
    "            #count word-tag       \n",
    "            for i in range(len(xtags)-1):\n",
    "                self.wordtag_counts[(xsentence[i],xtags[i])]+=1   \n",
    "\n",
    "            \n",
    "    def transition_proba(self, tag_tag): #transitions (tag1-tag2 pair count / tag1 count)  \n",
    "        tag_tag_count = self.tagtag_counts[tag_tag]\n",
    "        tag1_count = self.tag_counts[tag_tag[0]]\n",
    "        return tag_tag_count / tag1_count\n",
    "        \n",
    "    def emission_proba(self, word_tag): #emissions (word-tag pair counts / tag count).\n",
    "        ### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb0d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(God | B-person) 0.006052141527001317\n",
      "p(God | O) 0.000136064740049429\n",
      "p(Lindsay | B-person) 0.008047353019419334\n",
      "p(Lindsay | O) 2.230569509007033e-06\n"
     ]
    }
   ],
   "source": [
    "### ENTER CODE HERE ###\n",
    "print(f\"p(God | B-person) {hmm.emission_proba(('God','B-person'))}\")\n",
    "print(f\"p(God | O) {hmm.emission_proba(('God','O'))}\")\n",
    "print(f\"p(Lindsay | B-person) {hmm.emission_proba(('Lindsay','B-person'))}\")\n",
    "print(f\"p(Lindsay | O) {hmm.emission_proba(('Lindsay','O'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c8de3",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm used for finding the most likely sequence of hidden states in a hidden Markov model (HMM). It is widely used in various fields, including natural language processing (NLP), speech recognition, bioinformatics, and more.\n",
    "\n",
    "In NLP, the Viterbi algorithm is particularly important for part-of-speech tagging and named entity recognition tasks. These tasks involve determining the most likely sequence of grammatical categories (part-of-speech tags) or named entities (such as names of people, organizations, locations, etc.) that correspond to a given sequence of words in a sentence.\n",
    "\n",
    "We will create a Viterbi Function called `viterbi` that implements the viterbi algorithm. The function will take a list with a sentence String as the first element in the list and a list of tags as the second element in the list. \n",
    "\n",
    "It will return the sequence of BIO tags that gives the maximum score for that sentence as a list.\n",
    "\n",
    "#### Example\n",
    "\n",
    "**Input:**\n",
    "\n",
    "    [\"STOP WHAT YOU'RE DOING AND\", ['B-other', 'I-other', 'I-other', 'I-other', 'I-other']]\n",
    "    \n",
    "    \n",
    "**Output**\n",
    "\n",
    "    ['B-other', 'I-other', 'I-other', 'I-other', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b75012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(hmm, sent, printIt=''):\n",
    "    import itertools\n",
    "    import pprint\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    \n",
    "    sentence = sent[0].split()   \n",
    "    \n",
    "    tags = [i for i in hmm.tags if i not in ['<B>', '<E>']]    \n",
    "\n",
    "    table = []\n",
    "    for w in range(len(sentence)):        \n",
    "        word = sentence[w]        \n",
    "        for tag in tags:\n",
    "            #create structre and get emission value\n",
    "            \n",
    "            entry = {\"prev_key\"      :('' ,'', 0),                \n",
    "                     'prev_value'    :0, \n",
    "                     \"emission\"      :hmm.emission_proba((word, tag)),\n",
    "                     \"max_transition\":0, \n",
    "                     'value'         :0}\n",
    "            \n",
    "            #get the max transition value\n",
    "            if w ==0:\n",
    "                entry['max_transition'] = hmm.transition_proba(('<B>', tag))\n",
    "                entry['prev_key']       = ('<B>', '<B>', -1)\n",
    "                entry['prev_value']     = 0\n",
    "                entry['value']          = entry['max_transition'] * entry['emission'] \n",
    "\n",
    "            else:\n",
    "                transition_max = float(\"-inf\")\n",
    "                max_value = float(\"-inf\")\n",
    "                best_prev_entry = None\n",
    "\n",
    "                for prev_entry in [d for d in table if list(d.keys())[0][2] == w-1]: #all entries from previous levels\n",
    "                    prev_entry_tag = list(prev_entry.keys())[0][1]                    \n",
    "                    prev_entry_value = prev_entry[list(prev_entry.keys())[0]][\"value\"]\n",
    "\n",
    "                    transition = hmm.transition_proba((prev_entry_tag, tag))\n",
    "                    xvalue = entry[\"emission\"] * transition * prev_entry_value\n",
    "                    \n",
    "                    \n",
    "                    if xvalue >= max_value:\n",
    "                        transition_max = transition\n",
    "                        max_value = xvalue\n",
    "                        best_prev_entry = prev_entry\n",
    "\n",
    "                entry['max_transition'] = transition_max\n",
    "                entry['prev_key'] = list(best_prev_entry.keys())[0]\n",
    "                entry['prev_value'] = best_prev_entry[list(best_prev_entry.keys())[0]][\"value\"]\n",
    "                entry['value'] = entry['max_transition'] * entry['emission'] * entry['prev_value']\n",
    "            \n",
    "            table.append({(word, tag, w): entry})\n",
    "            \n",
    "    if printIt=='all': \n",
    "        for i in table:\n",
    "            pp.pprint(i)\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    #Backtrack\n",
    "    BIO_list = []\n",
    "    final_table = []\n",
    "    \n",
    "    #get the last level of the list:\n",
    "    filtered_table = [d for d in table if list(d.keys())[0][2] == len(sentence)-1]\n",
    "    \n",
    "    #get the last dict of our BIO_list with the higher value\n",
    "    current_dict = max(filtered_table, key=lambda d: list(d.values())[0]['value'])\n",
    "\n",
    "    #insert in the BIO list\n",
    "    BIO_list.insert(0, list(current_dict.keys())[0][1])\n",
    "    final_table.insert(0, current_dict)    \n",
    "    \n",
    "    while list(current_dict.values())[0]['prev_key'] != ('<B>', '<B>', -1): #while not the first element\n",
    "        current_dict = [d for d in table if list(current_dict.values())[0]['prev_key'] in d ][0]\n",
    "        BIO_list.insert(0, list(current_dict.keys())[0][1] )\n",
    "        final_table.insert(0, current_dict)\n",
    "        \n",
    "    if printIt=='result': \n",
    "        # Flatten dictionaries and convert to DataFrame\n",
    "        import pandas as pd\n",
    "        pd.set_option('display.max_columns', None)  # This option displays all columns\n",
    "        pd.set_option('display.expand_frame_repr', False)  # This option prevents pandas from wrapping tables over multiple lines\n",
    "        pd.set_option('display.max_rows', None)  # This option displays all rows\n",
    "        rows = []\n",
    "        for d in final_table:\n",
    "            for key, value in d.items():\n",
    "                # Flatten the keys and values\n",
    "                flat_dict = {**{\"key\": key}, **value}\n",
    "                rows.append(flat_dict)\n",
    "\n",
    "        df = pd.DataFrame(rows)        \n",
    "        print(df, \"\\n\")\n",
    "    \n",
    "    return BIO_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7093137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-other', 'I-other', 'O', 'O']\n",
      "['O', 'O', 'B-person', 'I-person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(viterbi(hmm,[\"Is sad that she 's missing Cowboy Mouth tonight :(\", ['O', 'O', 'O', 'O', 'O', 'O', 'B-musicartist', 'I-musicartist', 'O', 'O']]))\n",
    "print(viterbi(hmm,['is up and ready for his last day on the punt until Stakes Day ... #sadbuttrue', ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-other', 'I-other', 'O', 'O']]))\n",
    "print(viterbi(hmm,['@sfgiantsfan55 omg Todo Cabio has been my fav since like `07 i still listen to their CD all the time i want the new one . alejate de mi ...', ['O', 'O', 'B-person', 'I-person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079af9a",
   "metadata": {},
   "source": [
    "The dev data in `data/dev` is in the following format:\n",
    "\n",
    "    STOP\tO\n",
    "    WHAT\tO\n",
    "    YOU'RE\tO\n",
    "    DOING\tO\n",
    "    AND\tO\n",
    "    GO\tO\n",
    "    GET\tO\n",
    "    #ExpelledMovieToNumberOne\tO\n",
    "    ON\tO\n",
    "    ITUNES\tB-other\n",
    "    BECAUSE\tO\n",
    "    IT'S\tO\n",
    "    ONLY\tO\n",
    "    2ND\tO\n",
    "    !!\tO\n",
    "    @camerondallas\tO\n",
    "    Shs\tO\n",
    "    ...\n",
    "\n",
    "It is organized with two whitespace-seperated columns as seen above.\n",
    "\n",
    "Let's write code to read in the dev data and store it as list called `dev` in the same way that we read in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bbf64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = [' '.join(i.split('\\n')).replace('\\t', ' ').split() for i in open('data/dev', 'r').read().split('\\n\\n')]\n",
    "dev = [[\" \".join(i[::2]),i[1::2]] for i in dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f87264",
   "metadata": {},
   "source": [
    "The following is a python function inspired by **\"The conlleval script\"** written in Perl. It is often used to evaluate the performance of Named Entity Recognition (NER) models. It is named after the Conference on Natural Language Learning (CoNLL), where it was initially used for shared task evaluations.The conlleval script written in Perl is often used to evaluate the performance of Named Entity Recognition (NER) models. It is named after the Conference on Natural Language Learning (CoNLL), where it was initially used for shared task evaluations.\n",
    "\n",
    "Run the cell without modifying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf87529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "def conlleval(labels_predicted, labels_correct, labels_all):\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(labels_correct)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(labels_predicted)))\n",
    "\n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "\n",
    "    num_sentences = len(labels_predicted)\n",
    "    total_tokens = np.sum([len(s) for s in labels_predicted])\n",
    "    num_correct_sentences = sum([all(np.array(pred) == np.array(true)) for pred, true in zip(labels_predicted, labels_correct)])\n",
    "    correct_sentences_percentage = num_correct_sentences / num_sentences * 100\n",
    "    total_correct_tokens = sum([sum(np.array(pred) == np.array(true)) for pred, true in zip(labels_predicted, labels_correct)])\n",
    "    total_correct_tokens_percentage = total_correct_tokens / total_tokens * 100\n",
    "    tag_counts = {tag: sum([s.count(tag) for s in labels_predicted]) for tag in tagset}\n",
    "\n",
    "    classification_report_dict = classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "        output_dict=True,\n",
    "        zero_division=1,\n",
    "    )\n",
    "   \n",
    "    classification_report_dict.pop('macro avg', None)\n",
    "    classification_report_dict.pop('weighted avg', None)\n",
    "    classification_report_dict.pop('samples avg', None)\n",
    "    classification_report_dict.pop('micro avg', None)\n",
    "\n",
    "    total_precision = precision_score(y_true_combined, y_pred_combined, average='weighted', zero_division=1)\n",
    "    total_recall = recall_score(y_true_combined, y_pred_combined, average='weighted', zero_division=1)\n",
    "    total_f1 = f1_score(y_true_combined, y_pred_combined, average='weighted', zero_division=1)\n",
    "    total_line = f\"{'Total':<15s} {total_precision:<10.2f} {total_recall:<10.2f} {total_f1:<10.2f}\"\n",
    "\n",
    "    report_lines = [f\"{k:<15s} {classification_report_dict[k]['precision']:<10.2f} {classification_report_dict[k]['recall']:<10.2f} {classification_report_dict[k]['f1-score']:<10.2f}\" for k in classification_report_dict if isinstance(classification_report_dict[k], dict)]\n",
    "    report_lines.insert(0, \"\\n\")\n",
    "    report_lines.insert(1, f\"{'TAG':<15s} {'Precision':<10s} {'Recall':<10s} {'F1-score':<10s}\\n\")\n",
    "    report_lines.insert(2, total_line)\n",
    "    report_lines.insert(3, '-'*50 + '\\n') \n",
    "    classification_report_str = \"\\n\".join(report_lines)\n",
    "\n",
    "    additional_info_str = ''\n",
    "    additional_info_str += f'Total tokens: {total_tokens}\\n'\n",
    "    additional_info_str += f'Total correct tokens: {total_correct_tokens} ({total_correct_tokens_percentage:.2f}%)\\n'\n",
    "    additional_info_str += f'Processed sentences: {num_sentences}\\n'\n",
    "    additional_info_str += f'Completely correct sentences: {num_correct_sentences} ({correct_sentences_percentage:.2f}%)\\n'\n",
    "\n",
    "    return additional_info_str + classification_report_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769412fa",
   "metadata": {},
   "source": [
    "We can use the conlleval function to evaluate our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9228517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 16261\n",
      "Total correct tokens: 11491 (70.67%)\n",
      "Processed sentences: 1000\n",
      "Completely correct sentences: 281 (28.10%)\n",
      "\n",
      "\n",
      "TAG             Precision  Recall     F1-score  \n",
      "\n",
      "Total           0.92       0.71       0.79      \n",
      "--------------------------------------------------\n",
      "\n",
      "B-company       0.86       0.15       0.26      \n",
      "I-company       1.00       0.00       0.00      \n",
      "B-facility      0.19       0.08       0.11      \n",
      "I-facility      0.19       0.15       0.17      \n",
      "B-geo-loc       0.73       0.19       0.30      \n",
      "I-geo-loc       0.39       0.17       0.23      \n",
      "B-movie         0.00       0.00       0.00      \n",
      "I-movie         0.00       0.00       0.00      \n",
      "B-musicartist   0.00       0.00       0.00      \n",
      "I-musicartist   0.05       0.11       0.07      \n",
      "B-other         0.06       0.23       0.09      \n",
      "I-other         0.02       0.59       0.04      \n",
      "B-person        0.14       0.27       0.18      \n",
      "I-person        0.12       0.41       0.18      \n",
      "B-product       0.33       0.16       0.22      \n",
      "I-product       0.17       0.17       0.17      \n",
      "B-sportsteam    0.33       0.01       0.03      \n",
      "I-sportsteam    0.50       0.08       0.13      \n",
      "B-tvshow        0.00       0.00       0.00      \n"
     ]
    }
   ],
   "source": [
    "predictions =  [viterbi(hmm, sent) for sent in dev if len(sent[0])!=0]\n",
    "correct =      [d[1] for d in dev if len(d[1])>0]\n",
    "\n",
    "viterbiEvaluation = conlleval(predictions, correct, hmm.tags)\n",
    "print(viterbiEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a6d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
